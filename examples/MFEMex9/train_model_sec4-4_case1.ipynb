{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /g/g92/he10/.conda/envs/tfvenv/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n",
    "from error_utils import *\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle \n",
    "import copy\n",
    "import subprocess as sp\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'): free memory: 9768\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'): free memory: 15295\n",
      "PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'): free memory: 15295\n",
      "PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU'): free memory: 15295\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_memory():\n",
    "  _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "  ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "  COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "  memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "  memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "  return memory_free_values\n",
    "\n",
    "device_list = tf.config.list_physical_devices('GPU')\n",
    "free_mem = get_gpu_memory()\n",
    "for i,gpu in enumerate(device_list):\n",
    "    print(f'{gpu}: free memory: {free_mem[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which GPU to use\n",
    "config = tf.ConfigProto(log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True,visible_device_list='2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 0: params: [1.5 2. ], x shape: (301, 9216)\n",
      "case 1: params: [1.5 2.3], x shape: (301, 9216)\n",
      "case 2: params: [1.8 2. ], x shape: (301, 9216)\n",
      "case 3: params: [1.8 2.3], x shape: (301, 9216)\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "p1_train = np.linspace(1.5,1.8,2) # w1 of initial condition\n",
    "p2_train = np.linspace(2,2.3,2) # w2 of initial condition\n",
    "num_train = p1_train.size * p2_train.size  # number of training cases\n",
    "tstop = 3\n",
    "dt = 1e-2\n",
    "scaled = False # if data is normalized\n",
    "\n",
    "if num_train > 1:\n",
    "    train_data = pickle.load(open(f\"./data/local{num_train}_tstop{tstop:.1f}b.p\", \"rb\"))\n",
    "else:\n",
    "    train_data = pickle.load(open(f\"./data/local{num_train}_p1{p1_train[0]:.1f}_p2{p2_train[0]:.1f}_tstop{tstop:.1f}.p\", \"rb\"))\n",
    "num_sindy = len(train_data['data'])\n",
    "input_dim = train_data['data'][0]['x'].shape[1]\n",
    "\n",
    "# testing data\n",
    "# p1_test = p1_train\n",
    "# p2_test = p2_train\n",
    "p1_test = np.linspace(1.5,1.8,11) # w1 of initial condition\n",
    "p2_test = np.linspace(2,2.3,11) # w2 of initial condition\n",
    "num_test = p1_test.size * p2_test.size\n",
    "if num_test > 1:\n",
    "    test_data = pickle.load(open(f\"./data/local{num_test}_tstop{tstop:.1f}b.p\", \"rb\"))\n",
    "else:\n",
    "    test_data = pickle.load(open(f\"./data/local{num_test}_p1{p1_test[0]:.1f}_p2{p2_test[0]:.1f}_tstop{tstop:.1f}.p\", \"rb\"))\n",
    "\n",
    "for i in range(num_sindy):\n",
    "    print(f\"case {i}: params: {train_data['param'][i]}, x shape: {train_data['data'][i]['x'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 110, 120]\n"
     ]
    }
   ],
   "source": [
    "grid1, grid2 = np.meshgrid(p1_train, p2_train)\n",
    "train_param = np.hstack((grid1.flatten().reshape(-1,1), grid2.flatten().reshape(-1,1)))\n",
    "grid1, grid2 = np.meshgrid(p1_test, p2_test)\n",
    "test_param = np.hstack((grid1.flatten().reshape(-1,1), grid2.flatten().reshape(-1,1)))\n",
    "\n",
    "train_idx = []\n",
    "for i in range(num_test):\n",
    "    for j in range(num_train):\n",
    "        if np.abs(test_param[i,0]-train_param[j,0]) < 1e-8 and \\\n",
    "        np.abs(test_param[i,1]-train_param[j,1]) < 1e-8:\n",
    "            train_idx.append(i)\n",
    "print(train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaled:\n",
    "    x_min = train_data_x.min()\n",
    "    train_data_x -= x_min\n",
    "    x_max = train_data_x.max()\n",
    "    train_data_x /= x_max\n",
    "\n",
    "    dx_min = train_data_dx.min()\n",
    "    train_data_dx -= dx_min\n",
    "    dx_max = train_data_dx.max()\n",
    "    train_data_dx /= dx_max\n",
    "    \n",
    "    for i in range(num_train):\n",
    "        train_data['data'][i]['x'] = (train_data['data'][i]['x'] - x_min) / x_max\n",
    "        train_data['data'][i]['dx'] = (train_data['data'][i]['dx'] - dx_min) / dx_max\n",
    "    \n",
    "    for i in range(num_test):\n",
    "        test_data['data'][i]['x'] = (test_data['data'][i]['x'] - x_min) / x_max\n",
    "        test_data['data'][i]['dx'] = (test_data['data'][i]['dx'] - dx_min) / dx_max\n",
    "        \n",
    "else:\n",
    "    x_min = 0\n",
    "    x_max = 1\n",
    "    dx_min = 0\n",
    "    dx_max = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['seed'] = 1 # random seed\n",
    "params['config'] = config\n",
    "params['num_sindy'] = num_sindy\n",
    "params['param'] = train_data['param']\n",
    "params['train_idx'] = train_idx\n",
    "params['input_dim'] = input_dim\n",
    "params['latent_dim'] = 3\n",
    "params['model_order'] = 1\n",
    "params['poly_order'] = 1\n",
    "params['include_sine'] = False\n",
    "params['include_cosine'] = False\n",
    "params['include_costant'] = True\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], \n",
    "                                     params['include_sine'], params['include_cosine'], \n",
    "                                     params['include_costant'])\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = False\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_x'] = 1e-3\n",
    "params['loss_weight_sindy_z'] = 1e-3\n",
    "params['loss_weight_sindy_regularization'] = 0\n",
    "params['diff'] = 'symb' # 'symb': symbolic diff (only for fully connected Autoencoder), 'auto': automatic diff\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [100]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = train_data['data'][0]['x'].shape[0]\n",
    "params['batch_size'] = train_data['data'][0]['x'].shape[0]\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['fig_path'] = os.getcwd() + '/fig/nCase121b_tstop3_ld3_p1_1e-3_lr1e-3_width100_nDI25_upEP3e4_resNS0.1/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 100\n",
    "params['save_frequency'] = 100\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 1000000  # max number of training epochs\n",
    "params['refinement_epochs'] = 0\n",
    "\n",
    "# Greedy algorithm\n",
    "params['update_epoch'] = 30000 # update training set for every 2000 epochs\n",
    "params['tol'] = 0.001         # max error allowed in the parameter space\n",
    "params['tol2'] = 5            # max relative error allowed in the parameter space\n",
    "params['sindy_max'] = 25      # max number of local SINDys; if tolerance is used as a termination criterior, set it as None\n",
    "params['convex_knn'] = 1      # the number nearest local SINDys used for convex interpolation during Greedy sampling\n",
    "params['test_data'] = test_data # path of test data\n",
    "params['test_param'] = np.hstack((p1_test.reshape(-1,1), p2_test.reshape(-1,1)))    # parameters of test data\n",
    "params['num_test'] = num_test                     # number of test cases\n",
    "params['coeff_exist'] = False                     # flag to indicate whether to initialize model coefficients with pescribed values\n",
    "params['retrain'] = False     # whether to retrain the model\n",
    "\n",
    "# Error indicator:\n",
    "# 1: max relative error (if test data is available) \n",
    "# 2: residual norm (mean), 1D Burger's eqn\n",
    "# 3: residual norm (mean), 2D Burger's eqn\n",
    "# 4: MFEM example 16: Time dependent heat conduction\n",
    "# 5: MFEM example 9: DG advection\n",
    "params['err_type'] = 5                            \n",
    "params['subsize'] = int(0.3 * num_test)           # initial number of random testing cases for Greedy search\n",
    "params['subsize_max'] = 50                        # max percentage of random testing cases for Greedy search\n",
    "\n",
    "# Adaptive approach for tol of error indicator:\n",
    "# 'mean': use mean ratios between error indicator and max relative errors\n",
    "# 'reg_mean': use linear regression line\n",
    "# 'reg_max': use linear regression line shifted by std to upper bound\n",
    "# 'reg_min': use linear regression line shifted by std to lower bound, more conservative\n",
    "params['adaptive'] = 'reg_max'                      \n",
    "\n",
    "# PDE parameters\n",
    "params['pde'] = {}\n",
    "params['pde']['exe_file'] = '../../src/ex9'\n",
    "params['pde']['m_file'] = '../../src/periodic-square.mesh'\n",
    "params['pde']['u_file'] = './temp/ex9-u_pred1.gf'\n",
    "params['pde']['res_file'] = \"./temp/ex9-residual1.gf\"\n",
    "params['pde']['prob'] = 3\n",
    "params['pde']['rl'] = 4\n",
    "params['pde']['order'] = 1\n",
    "params['pde']['tstop'] = tstop\n",
    "params['pde']['dt'] = dt\n",
    "params['pde']['w1'] = p1_train[0]\n",
    "params['pde']['w2'] = p2_train[0]\n",
    "params['pde']['res_ns'] = 0.1 # percentage of time steps for residual evaluation\n",
    "params['pde']['Mmax_iter'] = 20 # max iterations in Tsolver\n",
    "\n",
    "params['scaled'] = scaled # if data is normalized\n",
    "params['x_min'] = x_min\n",
    "params['x_max'] = x_max\n",
    "params['dx_min'] = dx_min\n",
    "params['dx_max'] = dx_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['retrain']:\n",
    "    save_name = 'ex9_2022_02_06_20_13_22'\n",
    "    params = pickle.load(open(params['fig_path'] + save_name + '_params.pkl', 'rb'))\n",
    "    params['retrain'] = True\n",
    "    params['coeff_exist'] = True  # flag to indicate whether to initialize model coefficients with pescribed values\n",
    "    params['save_name'] = save_name\n",
    "    params['max_epochs'] = 1000000\n",
    "    params['update_epoch'] = 30000  # update training set for every 2000 epochs\n",
    "    params['save_frequency'] = 500\n",
    "    params['sindy_max'] = 25\n",
    "    \n",
    "    for i in params['train_idx'][4:]:\n",
    "        train_data['data'].append(test_data['data'][i])\n",
    "        train_data['param'].append(test_data['param'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run training experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 0\n",
      "* Evaluating\n",
      "  Time: 98.34 s, Case: [1.53 2.3 ], Tol: 0.00100, Max Error: 1.849433\n",
      "Epoch 0\n",
      "  train loss: 1.4151e+00, decoder: 1.0766e+00, sindy-x: 3.5449e-01, sindy-z: 3.3811e+02, sindy-reg: 4.0000\n",
      "Epoch 100\n",
      "  train loss: 1.1824e-01, decoder: 1.1377e-01, sindy-x: 3.3827e-01, sindy-z: 4.1326e+00, sindy-reg: 3.9720\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-442087f99514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ex9_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y_%m_%d_%H_%M_%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresults_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresults_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/WS1/he10/GreedySINDyAutoencoder/src/training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(training_data, params)\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder_network\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_biases'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'coeff_exist'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fig_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_params.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fig_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_experiments = 1\n",
    "df = pd.DataFrame()\n",
    "timer = []\n",
    "timer.append(time())\n",
    "for i in range(num_experiments):\n",
    "    print('EXPERIMENT %d' % i)\n",
    "    \n",
    "    if not params['retrain']:\n",
    "        params['save_name'] = 'ex9_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    tf.reset_default_graph()\n",
    "    results_dict = train_network(train_data, params)\n",
    "    df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "    \n",
    "timer.append(time())\n",
    "print(f'training time: {(timer[-1]-timer[0])/60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history of validation loss\n",
    "train_loss = np.array(df['training_losses'][0]).squeeze()\n",
    "test_loss = np.array(df['testing_losses'][0]).squeeze()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(9,5))\n",
    "xt = np.linspace(1,df['num_epochs'][0],train_loss.shape[0])\n",
    "ax1.plot(xt, train_loss[:,0], 'r', label='Train')\n",
    "ax1.set_yscale('log')\n",
    "ax1.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "ax1.set_xlabel('Epochs', fontsize=16)\n",
    "ax1.set_ylabel('Loss', color='r', fontsize=16)\n",
    "ax1.set_xlim(0, df['num_epochs'][0])\n",
    "ax1.tick_params(axis='x', labelsize=14)\n",
    "ax1.tick_params(axis='y', labelsize=16)\n",
    "ax1.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "xt = np.linspace(1,df['num_epochs'],test_loss.shape[0])\n",
    "ax2.plot(xt, test_loss, 'b-o', label='Val')\n",
    "ax2.set_ylabel('Max Error', color='b', fontsize=16)\n",
    "ax2.tick_params(axis='both', labelsize=16)\n",
    "ax2.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{params['fig_path']}/loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfvenv",
   "language": "python",
   "name": "tfvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
