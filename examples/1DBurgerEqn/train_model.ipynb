{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../src\")\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sindy_utils import library_size\n",
    "from training import train_network\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import pickle \n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "  _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "\n",
    "  ACCEPTABLE_AVAILABLE_MEMORY = 1024\n",
    "  COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "  memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "  memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "  return memory_free_values\n",
    "\n",
    "device_list = tf.config.list_physical_devices('GPU')\n",
    "free_mem = get_gpu_memory()\n",
    "for i,gpu in enumerate(device_list):\n",
    "    print(f'{gpu}: free memory: {free_mem[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify which GPU to use\n",
    "config = tf.ConfigProto(log_device_placement=False, gpu_options=tf.GPUOptions(allow_growth=True,\n",
    "                                                                              visible_device_list='0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load initial training dataset\n",
    "amp_train = np.linspace(0.7,0.9,2)\n",
    "width_train = np.linspace(0.9,1.1,2)\n",
    "num_train = amp_train.size * width_train.size # initial number of training data\n",
    "nx = 1001\n",
    "nt = 1000\n",
    "tstop = 1.0\n",
    "train_data = pickle.load(open(f\"/g/g92/he10/Research/data/1DBurgerEqn/local{num_train}.p\", \"rb\")) # A:[0.7,0.9], W:[0.9,1.1]\n",
    "num_sindy = len(train_data['data'])\n",
    "input_dim = train_data['data'][0]['x'].shape[1]\n",
    "\n",
    "for i in range(num_sindy):\n",
    "    print(f\"case {i}: params: {train_data['param'][i]}, x shape: {train_data['data'][i]['x'].shape}\")\n",
    "    \n",
    "# load dataset of discrete parameter space for greedy sampling\n",
    "amp_test = np.linspace(0.7,0.9,21)\n",
    "width_test = np.linspace(0.9,1.1,21)\n",
    "num_test = amp_test.size * width_test.size # number of cases in the discrete parameter space\n",
    "test_data = pickle.load(open(f\"/g/g92/he10/Research/data/1DBurgerEqn/local{num_test}.p\", \"rb\")) # A:[0.7,0.9], W:[0.9,1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid1, grid2 = np.meshgrid(amp_train, width_train)\n",
    "train_param = np.hstack((grid1.flatten().reshape(-1,1), grid2.flatten().reshape(-1,1)))\n",
    "grid1, grid2 = np.meshgrid(amp_test, width_test)\n",
    "test_param = np.hstack((grid1.flatten().reshape(-1,1), grid2.flatten().reshape(-1,1)))\n",
    "\n",
    "train_idx = []\n",
    "for i in range(num_test):\n",
    "    for j in range(num_train):\n",
    "        if np.abs(test_param[i,0]-train_param[j,0]) < 1e-8 and \\\n",
    "        np.abs(test_param[i,1]-train_param[j,1]) < 1e-8:\n",
    "            train_idx.append(i)\n",
    "print(train_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['seed'] = 1 # random seed\n",
    "params['config'] = config\n",
    "params['num_sindy'] = num_sindy\n",
    "params['param'] = train_data['param']\n",
    "params['train_idx'] = train_idx\n",
    "params['input_dim'] = input_dim\n",
    "params['latent_dim'] = 5\n",
    "params['poly_order'] = 1\n",
    "params['include_sine'] = False\n",
    "params['include_cosine'] = False\n",
    "params['include_costant'] = True\n",
    "params['library_dim'] = library_size(params['latent_dim'], params['poly_order'], \n",
    "                                     params['include_sine'], params['include_cosine'], \n",
    "                                     params['include_costant'])\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = False\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_x'] = 1e-1\n",
    "params['loss_weight_sindy_z'] = 1e-1\n",
    "params['loss_weight_sindy_regularization'] = 0\n",
    "params['diff'] = 'symb' # 'symb': symbolic diff (only for fully connected Autoencoder), 'auto': automatic diff\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [100]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = train_data['data'][0]['x'].shape[0]\n",
    "params['batch_size'] = 100\n",
    "params['learning_rate'] = 1e-3\n",
    "\n",
    "params['fig_path'] = os.getcwd() + '/fig/test/'\n",
    "if not os.path.exists(params['fig_path']):\n",
    "    os.makedirs(params['fig_path'])\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 100\n",
    "params['save_frequency'] = 100\n",
    "\n",
    "# training epochs\n",
    "params['max_epochs'] = 100000  # max number of training epochs\n",
    "\n",
    "# Greedy sampling algorithm\n",
    "params['update_epoch'] = 2000 # Greedy sampling frequency\n",
    "params['tol'] = 0.001         # initial tolerance of the maximum error indicator in the parameter space; it will be updated during training using the prescribed `adaptive` method\n",
    "params['tol2'] = 2            # initial tolerance of the maximum relative error in the parameter space\n",
    "params['sindy_max'] = 25      # max number of local DIs; if tolerance is used as a termination criterior, set it as None\n",
    "params['convex_knn'] = 1      # the number nearest local DIs used for convex interpolation during Greedy sampling\n",
    "params['test_data'] = test_data # dataset of the discrete parameter space\n",
    "params['test_param'] = np.hstack((amp_test.reshape(-1,1), width_test.reshape(-1,1)))    # parameters of the discrete parameter space\n",
    "params['num_test'] = num_test # the number of parameter cases of the discrete parameter space\n",
    "params['coeff_exist'] = False # whether to initialize model coefficients with pescribed values, set as False\n",
    "params['retrain'] = False     # whether to retrain the model; set as False for training a new model\n",
    "\n",
    "# Error indicator:\n",
    "# 1: max relative error (if test data is available); \n",
    "# 2: residual norm for 1D Burgers eqn; \n",
    "# 3: residual norm for 2D Burgers eqn; \n",
    "# 4: residual norm for time dependent heat conduction (MFEM example 16); \n",
    "# 5: residual norm for radial advection (MFEM example 9)\n",
    "params['err_type'] = 2                            \n",
    "params['subsize'] = int(0.5 * num_test) # initial random subset size, the number of randomly selected cases for Greedy sampling\n",
    "params['subsize_max'] = 80              # maximum random subset size in percentage\n",
    "\n",
    "# Adaptive approach for tol of error indicator:\n",
    "# 'mean': use mean ratios between error indicator and max relative errors\n",
    "# 'reg_mean': use linear regression line\n",
    "# 'reg_max': use linear regression line shifted by std to upper bound\n",
    "# 'reg_min': use linear regression line shifted by std to lower bound, more conservative\n",
    "params['adaptive'] = 'reg_max'                      \n",
    "\n",
    "# PDE parameters\n",
    "params['pde'] = {}\n",
    "params['pde']['nx'] = nx\n",
    "params['pde']['nt'] = nt\n",
    "params['pde']['tstop'] = tstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['retrain']:\n",
    "    save_name = 'burger_2021_11_26_10_02_50'\n",
    "    params = pickle.load(open(params['fig_path'] + save_name + '_params.pkl', 'rb'))\n",
    "    params['retrain'] = True\n",
    "    params['coeff_exist'] = True  # flag to indicate whether to initialize model coefficients with pescribed values\n",
    "    params['save_name'] = save_name\n",
    "    params['max_epochs'] = 5000\n",
    "    params['update_epoch'] = 5000\n",
    "    params['save_frequency'] = 1000\n",
    "    \n",
    "    for i in params['train_idx'][4:]:\n",
    "        train_data['data'].append(test_data['data'][i])\n",
    "        train_data['param'].append(test_data['param'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "timer = []\n",
    "timer.append(time())\n",
    "\n",
    "if not params['retrain']:\n",
    "    params['save_name'] = 'burger_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "tf.reset_default_graph()\n",
    "results_dict = train_network(train_data, params)\n",
    "df = df.append({**results_dict, **params}, ignore_index=True)\n",
    "    \n",
    "timer.append(time())\n",
    "print(f'training time: {(timer[-1]-timer[0])/60:.2f} mins, {(timer[-1]-timer[0])/3600:.2f} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history of validation loss\n",
    "train_loss = np.array(df['training_losses'][0]).squeeze()\n",
    "test_loss = np.array(df['testing_losses'][0]).squeeze()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(9,5))\n",
    "xt = np.linspace(1,df['epoch_count'],train_loss.shape[0])\n",
    "ax1.plot(xt, train_loss[:,0], 'r', label='Train')\n",
    "ax1.set_yscale('log')\n",
    "ax1.xaxis.set_major_formatter(FormatStrFormatter('%.f'))\n",
    "ax1.set_xlabel('Epochs', fontsize=16)\n",
    "ax1.set_ylabel('Loss', color='r', fontsize=16)\n",
    "ax1.set_xlim(0, params['epoch_count'])\n",
    "ax1.tick_params(axis='x', labelsize=14)\n",
    "ax1.tick_params(axis='y', labelsize=16)\n",
    "ax1.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "xt = np.linspace(1,df['epoch_count'],test_loss.shape[0])\n",
    "ax2.plot(xt, test_loss, 'b-o', label='Val')\n",
    "ax2.set_ylabel('Max Error', color='b', fontsize=16)\n",
    "ax2.tick_params(axis='both', labelsize=16)\n",
    "ax2.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{params['fig_path']}/loss.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfvenv",
   "language": "python",
   "name": "tfvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
